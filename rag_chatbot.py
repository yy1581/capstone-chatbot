import os
import pandas as pd
import time
import json
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.retrievers import BaseRetriever
from typing import List
import re

# ===== ì•Œë ˆë¥´ê¸° íƒì§€ í´ë˜ìŠ¤ =====

class AllergenDetector:
    """ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ì„ LLMìœ¼ë¡œ íƒì§€í•˜ëŠ” í´ë˜ìŠ¤ (100% LLM ê¸°ë°˜)"""
    
    # 19ê°€ì§€ ë²•ì • ì•Œë ˆë¥´ê¸° ìœ ë°œ ì‹í’ˆ (ì°¸ì¡°ìš©)
    ALLERGEN_CATEGORIES = [
        "ì•Œë¥˜", "ìš°ìœ ", "ë©”ë°€", "ë•…ì½©", "ëŒ€ë‘", "ë°€", "ì£", "í˜¸ë‘",
        "ê²Œ", "ìƒˆìš°", "ì˜¤ì§•ì–´", "ê³ ë“±ì–´", "ì¡°ê°œë¥˜", "ë³µìˆ­ì•„", "í† ë§ˆí† ",
        "ë‹­ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì‡ ê³ ê¸°", "ì•„í™©ì‚°ë¥˜"
    ]
    
    def __init__(self, llm):
        """
        Args:
            llm: Google Generative AI LLM ì¸ìŠ¤í„´ìŠ¤
        """
        self.llm = llm
    
    def detect(self, ingredients_text: str) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ì„ ì§ì ‘ ë¶„ì„í•˜ê³  íƒì§€.
        
        Args:
            ingredients_text: ì¬ë£Œ í…ìŠ¤íŠ¸
            
        Returns:
            íƒì§€ëœ ì•Œë ˆë¥´ê¸° ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
        """
        if pd.isna(ingredients_text) or str(ingredients_text).strip() == "":
            return []
        
        prompt = f"""ë‹¹ì‹ ì€ ì‹í’ˆ ì•Œë ˆë¥´ê¸° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì¬ë£Œ ëª©ë¡ì„ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ 19ê°€ì§€ ë²•ì • ì•Œë ˆë¥´ê¸° í•­ëª©ì— í•´ë‹¹í•˜ëŠ” ì„±ë¶„ì´ ìˆëŠ”ì§€ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.

**19ê°€ì§€ ë²•ì • ì•Œë ˆë¥´ê¸° í•­ëª©:**
ì•Œë¥˜, ìš°ìœ , ë©”ë°€, ë•…ì½©, ëŒ€ë‘, ë°€, ì£, í˜¸ë‘, ê²Œ, ìƒˆìš°, ì˜¤ì§•ì–´, ê³ ë“±ì–´, ì¡°ê°œë¥˜, ë³µìˆ­ì•„, í† ë§ˆí† , ë‹­ê³ ê¸°, ë¼ì§€ê³ ê¸°, ì‡ ê³ ê¸°, ì•„í™©ì‚°ë¥˜

**ì¬ë£Œ ëª©ë¡:**
{ingredients_text}

**ë¶„ì„ ì§€ì¹¨:**
1. ì§ì ‘ ì¬ë£Œ: ëª…ì‹œëœ ì¬ë£Œê°€ ì•Œë ˆë¥´ê¸° í•­ëª©ì¸ ê²½ìš° (ì˜ˆ: ê³„ë€, ìš°ìœ , ë°€ê°€ë£¨)
2. ê°€ê³µì‹í’ˆ ì›ì¬ë£Œ: ê°€ê³µì‹í’ˆì— í¬í•¨ëœ ì•Œë ˆë¥´ê¸° ì„±ë¶„ ë¶„ì„
   - ë§ˆìš”ë„¤ì¦ˆ, ë¨¸ë­, ì¼€ì´í¬ â†’ ì•Œë¥˜
   - ì¹˜ì¦ˆ, ìƒí¬ë¦¼, ë²„í„°, íœ˜í•‘í¬ë¦¼ â†’ ìš°ìœ 
   - ê°„ì¥, ëœì¥, ê³ ì¶”ì¥, ìŒˆì¥, ë‘ë¶€ â†’ ëŒ€ë‘
   - ë¹µ, ë¹µê°€ë£¨, íŒŒìŠ¤íƒ€, ë©´ë¥˜, íŠ€ê¹€ê°€ë£¨ â†’ ë°€
   - í–„, ë² ì´ì»¨, ì†Œì‹œì§€ â†’ ë¼ì§€ê³ ê¸° (ì£¼ë¡œ)
3. ìœ ì‚¬ í‘œí˜„: ë‹¤ì–‘í•œ í‘œí˜„ ì²´í¬
   - "ë‹¬ê±€"ê³¼ "ê³„ë€" â†’ ì•Œë¥˜
   - "ì†Œê³ ê¸°"ì™€ "ì‡ ê³ ê¸°" â†’ ì‡ ê³ ê¸°
   - "ì½©"ê³¼ "ëŒ€ë‘" â†’ ëŒ€ë‘
4. ì†ŒìŠ¤ì™€ ì–‘ë…: ì›ì¬ë£Œ ì¶”ì 
   - í¬ë¦¼ì†ŒìŠ¤, í™”ì´íŠ¸ì†ŒìŠ¤ â†’ ìš°ìœ 
   - ì¥ë¥˜(ê°„ì¥, ëœì¥, ê³ ì¶”ì¥) â†’ ëŒ€ë‘
5. í™•ì‹¤í•œ ê²½ìš°ë§Œ í¬í•¨í•˜ê³ , ì• ë§¤í•œ ê²½ìš°ëŠ” ì œì™¸

**ë‹µë³€ í˜•ì‹:**
- ë°œê²¬ëœ ì•Œë ˆë¥´ê¸° í•­ëª©ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´
- ì—†ìœ¼ë©´ 'ì—†ìŒ'ì´ë¼ê³ ë§Œ ë‹µë³€
- ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì •ë³´ ì—†ì´ í•­ëª©ëª…ë§Œ ì¶œë ¥

ì˜ˆì‹œ ë‹µë³€: ì•Œë¥˜, ìš°ìœ , ëŒ€ë‘, ë°€
"""
        
        try:
            response = self.llm.invoke(prompt)
            result = response.content if hasattr(response, 'content') else str(response)
            result = result.strip()
            
            if result == "ì—†ìŒ" or not result:
                return []
            
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì•Œë ˆë¥´ê¸° í•­ëª© íŒŒì‹±
            detected = [item.strip() for item in result.split(",")]
            # 19ê°€ì§€ ë²•ì • í•­ëª©ë§Œ í•„í„°ë§
            valid_allergens = [a for a in detected if a in self.ALLERGEN_CATEGORIES]
            return sorted(valid_allergens)
            
        except Exception as e:
            print(f"âš ï¸  LLM ì•Œë ˆë¥´ê¸° íƒì§€ ì˜¤ë¥˜: {e}")
            return []
    
    def detect_batch(self, ingredients_list: List[str], batch_size: int = 10) -> List[List[str]]:
        """
        ì—¬ëŸ¬ ì¬ë£Œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì†ë„ í–¥ìƒ.
        
        Args:
            ingredients_list: ì¬ë£Œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ì¬ë£Œ ìˆ˜
            
        Returns:
            ê° ì¬ë£Œì˜ ì•Œë ˆë¥´ê¸° ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i in range(0, len(ingredients_list), batch_size):
            batch = ingredients_list[i:i+batch_size]
            
            # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            batch_prompt = """ë‹¹ì‹ ì€ ì‹í’ˆ ì•Œë ˆë¥´ê¸° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì—¬ëŸ¬ ë ˆì‹œí”¼ì˜ ì¬ë£Œ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬ ê°ê°ì˜ ì•Œë ˆë¥´ê¸° ì„±ë¶„ì„ ì°¾ì•„ì£¼ì„¸ìš”.

**19ê°€ì§€ ë²•ì • ì•Œë ˆë¥´ê¸° í•­ëª©:**
ì•Œë¥˜, ìš°ìœ , ë©”ë°€, ë•…ì½©, ëŒ€ë‘, ë°€, ì£, í˜¸ë‘, ê²Œ, ìƒˆìš°, ì˜¤ì§•ì–´, ê³ ë“±ì–´, ì¡°ê°œë¥˜, ë³µìˆ­ì•„, í† ë§ˆí† , ë‹­ê³ ê¸°, ë¼ì§€ê³ ê¸°, ì‡ ê³ ê¸°, ì•„í™©ì‚°ë¥˜

**ë¶„ì„ ì§€ì¹¨:**
- ê°€ê³µì‹í’ˆ ì›ì¬ë£Œë„ í™•ì¸ (ë§ˆìš”ë„¤ì¦ˆâ†’ì•Œë¥˜, ê°„ì¥â†’ëŒ€ë‘, ë¹µê°€ë£¨â†’ë°€ ë“±)
- í™•ì‹¤í•œ ê²½ìš°ë§Œ í¬í•¨

**ì¬ë£Œ ëª©ë¡:**
"""
            for idx, ingredients in enumerate(batch, 1):
                batch_prompt += f"\n[{idx}] {ingredients[:200]}"  # ë„ˆë¬´ ê¸¸ë©´ 200ìë¡œ ì œí•œ
            
            batch_prompt += """

**ë‹µë³€ í˜•ì‹ (ê° ë²ˆí˜¸ë§ˆë‹¤ í•œ ì¤„ì”©):**
[1] ì•Œë¥˜, ìš°ìœ , ëŒ€ë‘
[2] ì—†ìŒ
[3] ë‹­ê³ ê¸°, ëŒ€ë‘
"""
            
            try:
                # RPM ì œí•œ ì¤€ìˆ˜: 10 RPM ì´í•˜ ìœ ì§€ (6ì´ˆ ëŒ€ê¸°)
                time.sleep(6.0)
                
                response = self.llm.invoke(batch_prompt)
                result = response.content if hasattr(response, 'content') else str(response)
                
                # ê²°ê³¼ íŒŒì‹±
                batch_results = []
                for line in result.strip().split('\n'):
                    if line.strip().startswith('['):
                        # [ìˆ«ì] ì´í›„ì˜ ë‚´ìš© ì¶”ì¶œ
                        content = line.split(']', 1)[1].strip() if ']' in line else ''
                        if content == 'ì—†ìŒ' or not content:
                            batch_results.append([])
                        else:
                            detected = [item.strip() for item in content.split(',')]
                            valid = [a for a in detected if a in self.ALLERGEN_CATEGORIES]
                            batch_results.append(sorted(valid))
                
                # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì±„ìš°ê¸°
                while len(batch_results) < len(batch):
                    batch_results.append([])
                
                results.extend(batch_results[:len(batch)])
                
            except Exception as e:
                print(f"âš ï¸  ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ë” ê¸´ ëŒ€ê¸° í›„ ì¬ì‹œë„
                time.sleep(10)
                # ì˜¤ë¥˜ ì‹œ ê° í•­ëª©ì— ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                results.extend([[] for _ in batch])
        
        return results


class FeedbackStore:
    """ì‚¬ìš©ì í”¼ë“œë°±ì„ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, feedback_file: str = "feedback_data.json"):
        self.feedback_file = feedback_file
        self.feedbacks = self._load_feedbacks()
    
    def _load_feedbacks(self):
        """ì €ì¥ëœ í”¼ë“œë°± ë¡œë“œ"""
        if os.path.exists(self.feedback_file):
            try:
                with open(self.feedback_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {"positive": [], "negative": []}
        return {"positive": [], "negative": []}
    
    def _save_feedbacks(self):
        """í”¼ë“œë°± ì €ì¥"""
        with open(self.feedback_file, 'w', encoding='utf-8') as f:
            json.dump(self.feedbacks, f, ensure_ascii=False, indent=2)
    
    def add_feedback(self, query: str, recipe_title: str, recipe_url: str, is_positive: bool):
        """í”¼ë“œë°± ì¶”ê°€"""
        feedback_entry = {
            "query": query,
            "recipe_title": recipe_title,
            "recipe_url": recipe_url,
            "timestamp": time.time()
        }
        
        if is_positive:
            self.feedbacks["positive"].append(feedback_entry)
            print("\nâœ… ê¸ì •ì ì¸ í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ê²€ìƒ‰ë¶€í„° ë°˜ì˜ë©ë‹ˆë‹¤!")
        else:
            self.feedbacks["negative"].append(feedback_entry)
            print("\nâŒ ë¶€ì •ì ì¸ í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ê²€ìƒ‰ ì‹œ ê°€ì¤‘ì¹˜ê°€ ë‚®ì•„ì§‘ë‹ˆë‹¤.")
        
        self._save_feedbacks()
    
    def get_recipe_score(self, recipe_url: str) -> float:
        """ë ˆì‹œí”¼ì˜ í”¼ë“œë°± ì ìˆ˜ ê³„ì‚° (ê¸ì •: +1, ë¶€ì •: -1)"""
        positive_count = sum(1 for fb in self.feedbacks["positive"] if fb["recipe_url"] == recipe_url)
        negative_count = sum(1 for fb in self.feedbacks["negative"] if fb["recipe_url"] == recipe_url)
        return positive_count - negative_count


class AllergenExtractor:
    """ì§ˆë¬¸ì—ì„œ ì•Œë ˆë¥´ê¸° ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    # ì•Œë ˆë¥´ê¸° í‚¤ì›Œë“œ ë§¤í•‘ (19ê°€ì§€ ë²•ì • ì•Œë ˆë¥´ê¸° ìœ ë°œ ì‹í’ˆ)
    ALLERGEN_KEYWORDS = {
        "ì•Œë¥˜": ["ì•Œ", "ì•Œë¥˜", "ê³„ë€", "ë‹¬ê±€", "ì—ê·¸", "ë‚œ"],
        "ìš°ìœ ": ["ìš°ìœ ", "ìœ ì œí’ˆ", "ì¹˜ì¦ˆ", "ë²„í„°", "ìƒí¬ë¦¼", "ìš”êµ¬ë¥´íŠ¸", "ëª¨ì§œë ë¼", "íŒŒë§ˆì‚°", "í¬ë¦¼"],
        "ë©”ë°€": ["ë©”ë°€", "ë©”ë°€êµ­ìˆ˜", "ì†Œë°”"],
        "ë•…ì½©": ["ë•…ì½©", "í”¼ë„›"],
        "ëŒ€ë‘": ["ëŒ€ë‘", "ë‘ë¶€", "ëœì¥", "ê°„ì¥", "ì½©", "ë‘ìœ ", "ì½©ë‚˜ë¬¼"],
        "ë°€": ["ë°€", "ë°€ê°€ë£¨", "ë¹µê°€ë£¨", "ë©´", "êµ­ìˆ˜", "íŒŒìŠ¤íƒ€", "ìš°ë™", "ë¼ë©´"],
        "ì£": ["ì£"],
        "í˜¸ë‘": ["í˜¸ë‘"],
        "ê²Œ": ["ê²Œ", "í‚¹í¬ë©", "ëŒ€ê²Œ"],
        "ìƒˆìš°": ["ìƒˆìš°", "í¬ë˜ë¯¸", "ìƒˆìš°ì “"],
        "ì˜¤ì§•ì–´": ["ì˜¤ì§•ì–´", "ê°‘ì˜¤ì§•ì–´", "í•œì¹˜"],
        "ê³ ë“±ì–´": ["ê³ ë“±ì–´", "ì‚¼ì¹˜", "ê½ì¹˜"],
        "ì¡°ê°œë¥˜": ["ì¡°ê°œ", "ì¡°ê°œë¥˜", "êµ´", "í™í•©", "ë°”ì§€ë½", "ëª¨ì‹œì¡°ê°œ", "ì „ë³µ", "ì†Œë¼"],
        "ë³µìˆ­ì•„": ["ë³µìˆ­ì•„"],
        "í† ë§ˆí† ": ["í† ë§ˆí† ", "ë°©ìš¸í† ë§ˆí† "],
        "ë‹­ê³ ê¸°": ["ë‹­ê³ ê¸°", "ì¹˜í‚¨", "ë‹­", "ë‹­ë‚ ê°œ", "ë‹­ë‹¤ë¦¬", "ë‹­ê°€ìŠ´ì‚´"],
        "ë¼ì§€ê³ ê¸°": ["ë¼ì§€ê³ ê¸°", "ì‚¼ê²¹ì‚´", "ëª©ì‚´", "ë“±ì‹¬", "ì•ë‹¤ë¦¬", "ë’·ë‹¤ë¦¬", "ë² ì´ì»¨", "í–„", "ë¼ì§€"],
        "ì‡ ê³ ê¸°": ["ì‡ ê³ ê¸°", "ì†Œê³ ê¸°", "í•œìš°", "ë“±ì‹¬", "ì•ˆì‹¬", "ìš°ë‘”", "ì–‘ì§€", "ì‚¬íƒœ", "ì†Œ"],
        "ì•„í™©ì‚°ë¥˜": ["ì•„í™©ì‚°", "ì•„í™©ì‚°ë¥˜", "ì´ì‚°í™”í™©"]
    }
    
    @staticmethod
    def extract_from_query(query: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ ì•Œë ˆë¥´ê¸° ì¬ë£Œ ì¶”ì¶œ (í˜•ì‹: ì•Œë ˆë¥´ê¸°: ì•Œë¥˜, ìš°ìœ  / ì§ˆë¬¸: ...)"""
        allergens = []
        
        # "ì•Œë ˆë¥´ê¸°:" í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
        allergen_pattern = r'ì•Œë ˆë¥´ê¸°\s*:\s*([^/]+)'
        match = re.search(allergen_pattern, query)
        
        if match:
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì•Œë ˆë¥´ê¸° í•­ëª© ì¶”ì¶œ
            allergen_text = match.group(1).strip()
            allergen_items = [item.strip() for item in allergen_text.split(',')]
            
            # 19ê°€ì§€ ë²•ì • ì•Œë ˆë¥´ê¸° í•­ëª©ë§Œ í—ˆìš©
            valid_allergens = list(AllergenExtractor.ALLERGEN_KEYWORDS.keys())
            for item in allergen_items:
                if item in valid_allergens:
                    allergens.append(item)
        
        return allergens
    
    @staticmethod
    def remove_allergen_keywords(query: str, allergens: List[str]) -> str:
        """ì§ˆë¬¸ì—ì„œ ì•Œë ˆë¥´ê¸° ê´€ë ¨ í‚¤ì›Œë“œ ì œê±° (í˜•ì‹: ì•Œë ˆë¥´ê¸°: ... / ì§ˆë¬¸: ...)"""
        # "ì•Œë ˆë¥´ê¸°: ... /" ë¶€ë¶„ ì™„ì „ ì œê±°
        cleaned_query = re.sub(r'ì•Œë ˆë¥´ê¸°\s*:[^/]*/\s*', '', query)
        
        # "ì§ˆë¬¸:" í…ìŠ¤íŠ¸ ì œê±°
        cleaned_query = re.sub(r'ì§ˆë¬¸\s*:\s*', '', cleaned_query)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        cleaned_query = re.sub(r'\s+', ' ', cleaned_query).strip()
        
        return cleaned_query


class FeedbackRetriever(BaseRetriever):
    """í”¼ë“œë°±ê³¼ ì•Œë ˆë¥´ê¸°ë¥¼ ê³ ë ¤í•œ Re-ranking Retriever"""
    
    base_retriever: object  # VectorStoreRetriever
    feedback_store: FeedbackStore
    allergen_detector: object  # AllergenDetector
    vectorstore: object  # FAISS VectorStore
    boost_factor: float = 0.3
    user_allergens: List[str] = []
    allergen_penalty_weight: float = 0.8  # ì•Œë ˆë¥´ê¸° í˜ë„í‹° ê°€ì¤‘ì¹˜
    
    def set_allergens(self, allergens: List[str]):
        """ì‚¬ìš©ì ì•Œë ˆë¥´ê¸° ì •ë³´ ì„¤ì •"""
        self.user_allergens = allergens
    
    def _calculate_allergen_similarity(self, doc: Document) -> float:
        """ì•Œë ˆë¥´ê¸° ì¬ë£Œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1, ë†’ì„ìˆ˜ë¡ ì•Œë ˆë¥´ê¸° ì¬ë£Œ í¬í•¨ ê°€ëŠ¥ì„± ë†’ìŒ)"""
        if not self.user_allergens:
            return 0.0
        
        # ë©”íƒ€ë°ì´í„°ì— ì €ì¥ëœ ì•Œë ˆë¥´ê¸° ì •ë³´ ìš°ì„  ì‚¬ìš©
        doc_allergens = doc.metadata.get('allergens', [])
        
        if doc_allergens:
            # ë©”íƒ€ë°ì´í„°ì˜ ì•Œë ˆë¥´ê¸° ì •ë³´ì™€ ë¹„êµ
            for allergen in self.user_allergens:
                if allergen in doc_allergens:
                    return 1.0  # ì •í™•í•œ ë§¤ì¹­
            return 0.0  # ì•Œë ˆë¥´ê¸° ì—†ìŒ
        
        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° (ë ˆê±°ì‹œ ë°ì´í„° ëŒ€ë¹„)
        # ì•Œë ˆë¥´ê¸° ì¹´í…Œê³ ë¦¬ëª…ì´ ì§ì ‘ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì²´í¬
        content_lower = doc.page_content.lower()
        for allergen in self.user_allergens:
            if allergen in content_lower:
                return 1.0  # ì•Œë ˆë¥´ê¸° ì„±ë¶„ í¬í•¨
        
        return 0.0
    
    def _get_relevant_documents(self, query: str) -> List[Document]:
        """í”¼ë“œë°±ê³¼ ì•Œë ˆë¥´ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ ë° Re-ranking"""
        # ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ë” ë§ì€ í›„ë³´ ê°€ì ¸ì˜¤ê¸°
        original_k = self.base_retriever.search_kwargs.get('k', 1)
        search_k = max(20, original_k * 20)  # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
        
        # ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        try:
            # FAISSì˜ similarity_search_with_score ì‚¬ìš©
            candidates_with_scores = self.vectorstore.similarity_search_with_score(query, k=search_k)
        except:
            # í´ë°±: ì¼ë°˜ ê²€ìƒ‰ ì‚¬ìš©
            self.base_retriever.search_kwargs = {'k': search_k}
            candidates = self.base_retriever.invoke(query)
            candidates_with_scores = [(doc, 0.0) for doc in candidates]
        
        # ê° ë¬¸ì„œì— ëŒ€í•´ ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì•Œë ˆë¥´ê¸° í•„í„°ë§
        scored_docs = []
        filtered_count = 0
        no_allergen_info_count = 0
        warning_docs = []  # ì•Œë ˆë¥´ê¸° ì •ë³´ ì—†ëŠ” ë¬¸ì„œë“¤ (ê²½ê³ ìš©)
        
        for doc, base_score in candidates_with_scores:
            recipe_url = doc.metadata.get('source', '')
            allergens = doc.metadata.get('allergens', [])
            
            # ì•Œë ˆë¥´ê¸° ì²´í¬ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
            allergen_similarity = self._calculate_allergen_similarity(doc)
            has_allergen = allergen_similarity >= 1.0  # ì •í™•í•œ ë§¤ì¹­ë§Œ ì œì™¸
            
            # ì•Œë ˆë¥´ê¸°ê°€ ìˆëŠ” ì²­í¬ëŠ” ì™„ì „íˆ ì œì™¸
            if has_allergen:
                filtered_count += 1
                continue
            
            # ì•Œë ˆë¥´ê¸° ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê²½ê³  í”Œë˜ê·¸ ì¶”ê°€
            has_no_allergen_info = (self.user_allergens and not allergens)
            if has_no_allergen_info:
                no_allergen_info_count += 1
            
            # 1. ê¸°ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜ (FAISSëŠ” L2 distance ë°˜í™˜, ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
            # L2 distanceë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ì ìˆ˜ ë†’ê²Œ)
            similarity_score = 1.0 / (1.0 + base_score)
            
            # 2. í”¼ë“œë°± ì ìˆ˜
            feedback_score = self.feedback_store.get_recipe_score(recipe_url)
            feedback_boost = feedback_score * self.boost_factor
            
            # 3. ë‹¤ì–‘ì„± ì ìˆ˜ (ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€ë¡œ ë§¤ë²ˆ ë‹¤ë¥¸ ë ˆì‹œí”¼ ì¶”ì²œ)
            import random
            diversity_noise = random.uniform(-0.05, 0.05)  # Â±5% ëœë¤ ë³€ë™
            
            # 4. ìµœì¢… ì ìˆ˜ = ê¸°ë³¸ ìœ ì‚¬ë„ + í”¼ë“œë°± ë³´ë„ˆìŠ¤ + ë‹¤ì–‘ì„± ë…¸ì´ì¦ˆ
            total_score = similarity_score + feedback_boost + diversity_noise
            
            scored_docs.append({
                'doc': doc,
                'base_score': similarity_score,
                'feedback_score': feedback_score,
                'diversity_noise': diversity_noise,
                'total_score': total_score,
                'has_allergen': False,
                'no_allergen_info': has_no_allergen_info  # ê²½ê³  í”Œë˜ê·¸
            })
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì¬ì •ë ¬
        scored_docs.sort(key=lambda x: x['total_score'], reverse=True)
        
        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        if self.user_allergens:
            print(f"\nğŸ“Š ì•Œë ˆë¥´ê¸° í•„í„°ë§ ê²°ê³¼:")
            print(f"  - ê²€ìƒ‰ëœ ì²­í¬: {len(candidates_with_scores)}ê°œ")
            print(f"  - ì•Œë ˆë¥´ê¸° ë§¤ì¹­: {filtered_count}ê°œ ì œì™¸")
            print(f"  - ì•Œë ˆë¥´ê¸° ì •ë³´ ì—†ìŒ: {no_allergen_info_count}ê°œ (âš ï¸ ê²½ê³  í¬í•¨)")
            print(f"  - ë‚¨ì€ ì²­í¬: {len(scored_docs)}ê°œ")
            
            if scored_docs:
                print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ 3ê°œ (ì•Œë ˆë¥´ê¸° í•„í„°ë§ í›„):")
                for i, item in enumerate(scored_docs[:3], 1):
                    title = item['doc'].metadata.get('title', 'ì œëª© ì—†ìŒ')[:50]
                    allergens = item['doc'].metadata.get('allergens', [])
                    allergen_str = ', '.join(allergens) if allergens else 'âš ï¸ ì •ë³´ ì—†ìŒ'
                    warning = " [ì£¼ì˜í•„ìš”]" if item.get('no_allergen_info', False) else ""
                    print(f"  {i}. {title}{warning}")
                    print(f"     ìœ ì‚¬ë„: {item['base_score']:.3f} | "
                          f"ì•Œë ˆë¥´ê¸°: {allergen_str} | "
                          f"ìµœì¢…: {item['total_score']:.3f}")
            
            # í•„í„°ë§ëœ ë¬¸ì„œ ì˜ˆì‹œ ì¶œë ¥ (ìƒìœ„ 3ê°œ)
            if filtered_count > 0:
                print(f"\nğŸš« í•„í„°ë§ëœ ë¬¸ì„œ ì˜ˆì‹œ (ìƒìœ„ 3ê°œ):")
                filtered_shown = 0
                for doc, base_score in candidates_with_scores:
                    allergen_similarity = self._calculate_allergen_similarity(doc)
                    if allergen_similarity >= 1.0:
                        title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')[:50]
                        allergens = doc.metadata.get('allergens', [])
                        matched = [a for a in allergens if a in self.user_allergens]
                        print(f"  {filtered_shown + 1}. {title}")
                        print(f"     ì•Œë ˆë¥´ê¸°: {', '.join(allergens)} | ë§¤ì¹­: {', '.join(matched)}")
                        filtered_shown += 1
                        if filtered_shown >= 3:
                            break
        
        # ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ê²½ê³ 
        if len(scored_docs) < original_k:
            print(f"\nâš ï¸ ì•Œë ˆë¥´ê¸° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë ˆì‹œí”¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ({len(scored_docs)}/{original_k}ê°œ)")
        
        return [item['doc'] for item in scored_docs[:original_k]]


class VectorStoreBuilder:
    """ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤ (100% LLM ê¸°ë°˜ ì•Œë ˆë¥´ê¸° íƒì§€)"""
    
    def __init__(self, embeddings, persist_directory: str = "faiss_recipe_index"):
        self.embeddings = embeddings
        self.persist_directory = persist_directory
        # LLM ì´ˆê¸°í™” (Gemini 2.5 Flash)
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.3
        )
        # ì•Œë ˆë¥´ê¸° íƒì§€ê¸° ì´ˆê¸°í™” (LLM ì „ë‹¬)
        self.allergen_detector = AllergenDetector(self.llm)
    
    def build_from_csv(self, csv_path: str):
        """CSV íŒŒì¼ì—ì„œ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
        print(f"ë ˆì‹œí”¼ ë°ì´í„° ë¡œë”© ì¤‘... ({csv_path})")
        df = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)
        print(f"ë ˆì‹œí”¼ ê°œìˆ˜: {len(df)}")
        
        # ë¬¸ì„œ ìƒì„±
        documents = self._create_documents_from_dataframe(df)
        print(f"ìƒì„±ëœ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = text_splitter.split_documents(documents)
        print(f"ë¶„í• ëœ ì²­í¬ ê°œìˆ˜: {len(split_docs)}")
        
        # ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
        vectorstore = self._build_with_checkpoints(split_docs)
        
        # ì €ì¥
        vectorstore.save_local(self.persist_directory)
        print(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ: {self.persist_directory}")
        
        return vectorstore
    
    def _create_documents_from_dataframe(self, df):
        """DataFrameì—ì„œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„± (LLM ë°°ì¹˜ ê¸°ë°˜ ì•Œë ˆë¥´ê¸° ë¶„ì„)"""
        documents = []
        total_recipes = len(df)
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        checkpoint_file = "allergen_detection_checkpoint.json"
        
        print("ğŸ¤– LLM ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëª¨ë“  ë ˆì‹œí”¼ì˜ ì•Œë ˆë¥´ê¸° ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘...")
        print("âš¡ 1000ê°œì”© ì´ˆëŒ€ìš©ëŸ‰ ë°°ì¹˜ + 2ì´ˆ ëŒ€ê¸°ë¡œ ì†ë„ 3ë°° í–¥ìƒ!")
        print("â³ ì˜ˆìƒ ì‹œê°„: í•˜ë£¨ ì•½ 250,000ê°œ ì²˜ë¦¬ ê°€ëŠ¥ (250 RPD Ã— 1000ê°œ/ë°°ì¹˜)")
        print("ğŸ’¾ ì§„í–‰ ìƒí™©ì€ ìë™ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤ (ì¤‘ë‹¨ ì‹œ ì´ì–´ì„œ ì§„í–‰ ê°€ëŠ¥)")
        
        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        all_allergens = []
        start_idx = 0
        
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                    all_allergens = checkpoint_data.get('allergens', [])
                    start_idx = len(all_allergens)
                    print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {start_idx}ê°œ ë ˆì‹œí”¼ ì´ë¯¸ ì²˜ë¦¬ë¨ (ì´ì–´ì„œ ì§„í–‰)")
            except Exception as e:
                print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}, ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                all_allergens = []
                start_idx = 0
        
        # ë‚¨ì€ ì¬ë£Œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (start_idxë¶€í„°)
        ingredients_list = []
        if start_idx < len(df):
            for idx in range(start_idx, len(df)):
                row = df.iloc[idx]
                ingredients = row['ì¬ë£Œ']
                if pd.notna(ingredients) and str(ingredients).strip():
                    ingredients_list.append(str(ingredients))
                else:
                    ingredients_list.append("")
        
        if ingredients_list:
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì•Œë ˆë¥´ê¸° íƒì§€
            print(f"\nğŸ“¦ ë‚¨ì€ {len(ingredients_list)}ê°œ ë ˆì‹œí”¼ë¥¼ 1000ê°œì”© ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
            batch_size = 1000
            remaining_allergens = []
            
            for batch_start in range(0, len(ingredients_list), batch_size):
                batch_end = min(batch_start + batch_size, len(ingredients_list))
                batch = ingredients_list[batch_start:batch_end]
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_results = self.allergen_detector.detect_batch(batch, batch_size=len(batch))
                remaining_allergens.extend(batch_results)
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                current_total = start_idx + len(remaining_allergens)
                if current_total % 1000 == 0 or batch_end == len(ingredients_list):
                    print(f"ì§„í–‰ ì¤‘: {current_total}/{total_recipes} ë ˆì‹œí”¼ ì²˜ë¦¬ ì™„ë£Œ ({current_total*100//total_recipes}%)")
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (1000ê°œë§ˆë‹¤)
                if len(remaining_allergens) % 1000 == 0 or batch_end == len(ingredients_list):
                    temp_allergens = all_allergens + remaining_allergens
                    try:
                        with open(checkpoint_file, 'w', encoding='utf-8') as f:
                            json.dump({'allergens': temp_allergens, 'total': total_recipes}, f, ensure_ascii=False)
                        print(f"  ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {len(temp_allergens)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
                    except Exception as e:
                        print(f"  âš ï¸  ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            all_allergens.extend(remaining_allergens)
        else:
            print(f"âœ… ëª¨ë“  ë ˆì‹œí”¼({len(all_allergens)}ê°œ) ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œ!")
        
        # Document ìƒì„±
        print("\nğŸ“„ Document ê°ì²´ ìƒì„± ì¤‘...")
        for idx, row in df.iterrows():
            if (idx + 1) % 1000 == 0 or idx == 0:
                print(f"ì§„í–‰ ì¤‘: {idx + 1}/{total_recipes} ë ˆì‹œí”¼ ì²˜ë¦¬ ì¤‘...")
            
            content = (
                f"ìš”ë¦¬ ì œëª©: {row['ì œëª©']}\n\n"
                f"ì¬ë£Œ: {row['ì¬ë£Œ']}\n\n"
                f"ì¸ë¶„: {row.get('ì¸ë¶„', '')}\n\n"
                f"ì†Œê°œ: {row['ì¸íŠ¸ë¡œ']}\n\n"
                f"ì¡°ë¦¬ ìˆœì„œ: {row['ì¡°ë¦¬ìˆœì„œ']}"
            )
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì–»ì€ ì•Œë ˆë¥´ê¸° ì •ë³´ ì‚¬ìš©
            detected_allergens = all_allergens[idx] if idx < len(all_allergens) else []
            
            # ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 10ê°œë§Œ)
            if idx < 10 and detected_allergens:
                print(f"  â””â”€ [{row['ì œëª©'][:20]}...] ì•Œë ˆë¥´ê¸°: {', '.join(detected_allergens)}")
            
            documents.append(Document(
                page_content=content,
                metadata={
                    "index": row.get('index', ''),
                    "ì¢…ë¥˜ë³„": row.get('ì¢…ë¥˜ë³„', ''),
                    "ìƒí™©ë³„": row.get('ìƒí™©ë³„', ''),
                    "ì¬ë£Œë³„": row.get('ì¬ë£Œë³„', ''),
                    "ë°©ë²•ë³„": row.get('ë°©ë²•ë³„', ''),
                    "title": row['ì œëª©'],
                    "source": row['url'],
                    "ì¡°íšŒìˆ˜": row.get('ì¡°íšŒìˆ˜', ''),
                    "ì…°í”„": row.get('ì…°í”„', ''),
                    "servings": row.get('ì¸ë¶„', ''),
                    "ì¡°ë¦¬ì‹œê°„": row.get('ì¡°ë¦¬ì‹œê°„', ''),
                    "ë‚œì´ë„": row.get('ë‚œì´ë„', ''),
                    "ingredients": row['ì¬ë£Œ'],
                    "ì¸íŠ¸ë¡œ": row.get('ì¸íŠ¸ë¡œ', ''),
                    "ì¡°ë¦¬ìˆœì„œ": row.get('ì¡°ë¦¬ìˆœì„œ', ''),
                    "í•´ì‹œíƒœê·¸": row.get('í•´ì‹œíƒœê·¸', ''),
                    "AIë¦¬ë·°ìš”ì•½": row.get('AIë¦¬ë·°ìš”ì•½', ''),
                    "allergens": detected_allergens  # ì•Œë ˆë¥´ê¸° ì •ë³´ ì¶”ê°€
                }
            ))
        
        print(f"âœ… ì´ {total_recipes}ê°œ ë ˆì‹œí”¼ ë¡œë“œ ì™„ë£Œ (ì•Œë ˆë¥´ê¸° ì •ë³´ í¬í•¨)!")
        return documents
    
    def _build_with_checkpoints(self, split_docs):
        """ì²´í¬í¬ì¸íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
        # ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        checkpoint_files = []
        for filename in os.listdir('.'):
            if filename.startswith(f"{self.persist_directory}_checkpoint_"):
                try:
                    num = int(filename.split('_')[-1])
                    checkpoint_files.append((num, filename))
                except:
                    pass
        
        vectorstore = None
        start_index = 0
        
        if checkpoint_files:
            checkpoint_files.sort(reverse=True)
            start_index, checkpoint_path = checkpoint_files[0]
            print(f"\nğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬! {checkpoint_path}ì—ì„œ ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
            print(f"   ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬: {start_index:,}ê°œ")
            print(f"   ë‚¨ì€ ì²­í¬: {len(split_docs) - start_index:,}ê°œ")
            vectorstore = FAISS.load_local(
                checkpoint_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
        else:
            print("\nğŸ†• ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ì„ë² ë”© ìƒì„±
        if start_index < len(split_docs):
            remaining_docs = len(split_docs) - start_index
            print(f"\nì„ë² ë”©ì„ ìƒì„±í•˜ê³  ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶• ì¤‘ì…ë‹ˆë‹¤...")
            print(f"ì´ {remaining_docs:,}ê°œì˜ ì²­í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ì „ì²´: {len(split_docs):,}ê°œ)")
            
            batch_size = 1000
            save_interval = 10000
            start_time = time.time()
            
            for i in range(start_index, len(split_docs), batch_size):
                batch = split_docs[i:i+batch_size]
                if vectorstore is None:
                    vectorstore = FAISS.from_documents(batch, self.embeddings)
                else:
                    vectorstore.add_documents(batch)
                
                processed = min(i + batch_size, len(split_docs))
                progress = (processed / len(split_docs)) * 100
                elapsed_time = time.time() - start_time
                processed_in_session = processed - start_index
                
                if processed < len(split_docs) and processed_in_session > 0:
                    estimated_total_time = (elapsed_time / processed_in_session) * remaining_docs
                    remaining_time = estimated_total_time - elapsed_time
                    remaining_hours = int(remaining_time // 3600)
                    remaining_minutes = int((remaining_time % 3600) // 60)
                    print(
                        f"ì§„í–‰: {processed:,}/{len(split_docs):,} ({progress:.1f}%) - "
                        f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining_hours}ì‹œê°„ {remaining_minutes}ë¶„"
                    )
                else:
                    total_time = elapsed_time
                    hours = int(total_time // 3600)
                    minutes = int((total_time % 3600) // 60)
                    print(
                        f"âœ… ì™„ë£Œ: {processed:,}/{len(split_docs):,} (100%) - "
                        f"ì´ ì†Œìš” ì‹œê°„: {hours}ì‹œê°„ {minutes}ë¶„"
                    )
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if (processed > start_index and 
                    processed % save_interval == 0 and 
                    processed < len(split_docs)):
                    checkpoint_path = f"{self.persist_directory}_checkpoint_{processed}"
                    print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì¤‘... ({processed:,}ê°œ ì²˜ë¦¬ë¨) â†’ {checkpoint_path}")
                    try:
                        vectorstore.save_local(checkpoint_path)
                        print(f"âœ… ì¤‘ê°„ ì €ì¥ ì™„ë£Œ!")
                    except Exception as e:
                        print(f"âš ï¸ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return vectorstore


class RagChatbot:
    """RAG ê¸°ë°˜ ë ˆì‹œí”¼ ì±—ë´‡ ë©”ì¸ í´ë˜ìŠ¤ (100% LLM ê¸°ë°˜ ì•Œë ˆë¥´ê¸° íƒì§€)"""
    
    def __init__(self, csv_path: str = "data/recipe_main_5.csv", 
                 faiss_index_path: str = "faiss_recipe_index"):
        self.csv_path = csv_path
        self.faiss_index_path = faiss_index_path
        
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        if not os.getenv("GOOGLE_API_KEY"):
            raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print("ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")
        self.embeddings = self._load_embeddings()
        
        # LLM ì´ˆê¸°í™”
        print("LLMì„ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤...")
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
        
        # ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        self.allergen_detector = AllergenDetector(self.llm)
        
        # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ë˜ëŠ” êµ¬ì¶•
        if os.path.exists(faiss_index_path):
            print("ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")
            self.vectorstore = FAISS.load_local(
                faiss_index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
        else:
            print("ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
            builder = VectorStoreBuilder(self.embeddings, faiss_index_path)
            self.vectorstore = builder.build_from_csv(csv_path)
        
        # Feedback ì €ì¥ì†Œ ë° Retriever ì´ˆê¸°í™”
        print("í”¼ë“œë°± ì €ì¥ì†Œë¥¼ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤...")
        self.feedback_store = FeedbackStore()
        
        base_retriever = self.vectorstore.as_retriever(search_kwargs={'k': 3})  # 3ê°œ ë ˆì‹œí”¼ ì¶”ì²œ
        self.retriever = FeedbackRetriever(
            base_retriever=base_retriever,
            feedback_store=self.feedback_store,
            allergen_detector=self.allergen_detector,
            vectorstore=self.vectorstore,  # FAISS vectorstore ì „ë‹¬
            boost_factor=0.3,
            allergen_penalty_weight=0.8  # ì•Œë ˆë¥´ê¸° í˜ë„í‹° ê°€ì¤‘ì¹˜
        )
        
        # RAG ì²´ì¸ ë° LLM ì „ìš© ì²´ì¸ êµ¬ì„±
        self.rag_chain = self._build_rag_chain()
        self.llm_only_chain = self._build_llm_only_chain()
        
        print("\nâœ… RAG ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    def _load_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        model_name = "jhgan/ko-sroberta-multitask"
        model_kwargs = {'device': 'cpu'}
        encode_kwargs = {'normalize_embeddings': True}
        return HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
    
    def _format_docs_with_info(self, docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œì— ì•Œë ˆë¥´ê¸° ì •ë³´ ì¶”ê°€"""
        formatted_docs = []
        
        for doc in docs:
            content = doc.page_content
            servings = doc.metadata.get('servings', 2)
            title = doc.metadata.get('title', '')
            source = doc.metadata.get('source', '')
            
            # ì•Œë ˆë¥´ê¸° ì •ë³´ (ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)
            allergens = doc.metadata.get('allergens', [])
            allergen_text = ", ".join(allergens) if allergens else "ì—†ìŒ"
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            metadata_text = f"\n\n=== ë ˆì‹œí”¼ ì •ë³´ ===\n"
            metadata_text += f"ì œëª©: {title}\n"
            metadata_text += f"ì¶œì²˜: {source}\n"
            
            if doc.metadata.get('ì¡°íšŒìˆ˜'):
                metadata_text += f"ì¡°íšŒìˆ˜: {doc.metadata['ì¡°íšŒìˆ˜']}\n"
            if doc.metadata.get('ì…°í”„'):
                metadata_text += f"ì‘ì„±ì: {doc.metadata['ì…°í”„']}\n"
            if doc.metadata.get('ì¡°ë¦¬ì‹œê°„'):
                metadata_text += f"ì¡°ë¦¬ì‹œê°„: {doc.metadata['ì¡°ë¦¬ì‹œê°„']}\n"
            if doc.metadata.get('ë‚œì´ë„'):
                metadata_text += f"ë‚œì´ë„: {doc.metadata['ë‚œì´ë„']}\n"
            
            metadata_text += f"\nì¸ë¶„: {servings}\n"
            metadata_text += f"ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„: {allergen_text}\n"
            
            cooking_steps = doc.metadata.get('ì¡°ë¦¬ìˆœì„œ', '')
            if cooking_steps and cooking_steps != 'nan':
                metadata_text += f"\nì¡°ë¦¬ ìˆœì„œ:\n{cooking_steps}\n"
            
            full_content = content + metadata_text
            formatted_docs.append(full_content)
        
        return "\n\n" + "="*50 + "\n\n".join(formatted_docs)
    
    def _build_rag_chain(self):
        """RAG ì²´ì¸ êµ¬ì„±"""
        prompt_template = """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìš”ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë ˆì‹œí”¼ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì§€ì¹¨:**
1. ê²€ìƒ‰ëœ ì—¬ëŸ¬ ë ˆì‹œí”¼ ì¤‘ì—ì„œ ì§ˆë¬¸ì— **ê°€ì¥ ì í•©í•œ ë ˆì‹œí”¼ 1ê°œ**ë¥¼ ì„ íƒí•˜ì—¬ ì¶”ì²œí•˜ì„¸ìš”
2. ì‚¬ìš©ìê°€ ì•Œë ˆë¥´ê¸° ì •ë³´ë¥¼ ëª…ì‹œí•œ ê²½ìš°, í•´ë‹¹ ì¬ë£Œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ë ˆì‹œí”¼ë§Œ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤
3. ë ˆì‹œí”¼ì˜ ì£¼ìš” íŠ¹ì§•, ì¬ë£Œ, ì¡°ë¦¬ë²•ì„ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”
4. ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ê³¼ ì˜ì–‘ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
5. ì‚¬ìš©ìê°€ ì•Œë ˆë¥´ê¸°ë¥¼ ëª…ì‹œí–ˆë‹¤ë©´, í•´ë‹¹ ì¬ë£Œê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŒì„ í™•ì‹¤íˆ ì•ˆë‚´í•˜ì„¸ìš”
6. **âš ï¸ ì•Œë ˆë¥´ê¸° ì •ë³´ê°€ ì—†ëŠ” ë ˆì‹œí”¼ëŠ” "ì•Œë ˆë¥´ê¸° ì •ë³´ ë¯¸í™•ì¸" ê²½ê³ ë¥¼ ë°˜ë“œì‹œ í‘œì‹œí•˜ì„¸ìš”**
7. ì¶œì²˜ URLì„ ì œê³µí•˜ì„¸ìš”
8. ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”

**ì¤‘ìš”:** ê²€ìƒ‰ëœ ë ˆì‹œí”¼ ì¤‘ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ **ì œëª©ì´ ì •í™•íˆ ì¼ì¹˜**í•˜ê±°ë‚˜ ê°€ì¥ ìœ ì‚¬í•œ ë ˆì‹œí”¼ë¥¼ ìš°ì„  ì„ íƒí•˜ì„¸ìš”.
ì˜ˆ: "ë¬´êµ­"ì„ ë¬¼ì—ˆë‹¤ë©´ "ë¬´ë‚˜ë¬¼ë³¶ìŒ"ë³´ë‹¤ "ë¬´êµ­ ë“ì´ê¸°"ë¥¼ ì„ íƒ

**ê²€ìƒ‰ëœ ë ˆì‹œí”¼ ì •ë³´:**
{context}

**ì‚¬ìš©ì ì§ˆë¬¸:**
{question}

**ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ:**
[ë ˆì‹œí”¼ëª…]ì„ ì¶”ì²œí•©ë‹ˆë‹¤!
(ì•Œë ˆë¥´ê¸° ì •ë³´ê°€ ìˆì—ˆë‹¤ë©´) âœ… ì´ ë ˆì‹œí”¼ì—ëŠ” [ì•Œë ˆë¥´ê¸° ì¬ë£Œ]ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.

[ê°„ë‹¨í•œ ì„¤ëª…]

**ì¬ë£Œ:**
- ...

**ì¡°ë¦¬ ë°©ë²•:**
1. ...

**ì¤‘ìš” ì •ë³´:**
- ì•Œë ˆë¥´ê¸°: ...
- ì˜ì–‘ì •ë³´(1ì¸ë¶„): ...
- ì¶œì²˜: [URL]

[ê°„ë‹¨í•œ ì¡°ë¦¬ ì„¤ëª…]

ë‹¤ë¥¸ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!

ë‹µë³€:"""
        
        prompt = ChatPromptTemplate.from_template(prompt_template)
        
        # ê²€ìƒ‰ê³¼ ì§ˆë¬¸ì„ ë¶„ë¦¬í•˜ëŠ” ì²´ì¸
        def retrieve_with_query(inputs):
            """ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰, ì›ë³¸ ì§ˆë¬¸ì€ ìœ ì§€"""
            search_query = inputs.get("search_query", inputs.get("question"))
            question = inputs.get("question")
            docs = self.retriever.invoke(search_query)
            return {
                "context": self._format_docs_with_info(docs),
                "question": question
            }
        
        rag_chain = (
            retrieve_with_query
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        return rag_chain
    
    def _build_llm_only_chain(self):
        """LLMë§Œ ì‚¬ìš©í•˜ëŠ” ì²´ì¸ êµ¬ì„± (RAG ì—†ìŒ)"""
        prompt_template = """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìš”ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì§€ì¹¨:**
1. ì¼ë°˜ì ì¸ ìš”ë¦¬ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”

**ì‚¬ìš©ì ì§ˆë¬¸:**
{question}

ë‹µë³€:"""
        
        prompt = ChatPromptTemplate.from_template(prompt_template)
        
        llm_only_chain = (
            {"question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        return llm_only_chain
    
    def run(self, mode='compare'):
        """ì±—ë´‡ ì‹¤í–‰
        
        Args:
            mode: 'rag' (RAGë§Œ ì‚¬ìš©), 'llm' (LLMë§Œ ì‚¬ìš©), 'compare' (ë¹„êµ ëª¨ë“œ)
        """
        print("\n" + "="*60)
        if mode == 'compare':
            print("ğŸ“Š RAG vs LLM ë¹„êµ ëª¨ë“œ")
            print("="*60)
            print("ê° ì§ˆë¬¸ì— ëŒ€í•´ RAGì™€ LLM ì „ìš© ëª¨ë“œì˜ ë‹µë³€ì„ ë¹„êµí•©ë‹ˆë‹¤.")
        elif mode == 'rag':
            print("ğŸ” RAG ëª¨ë“œ (ë ˆì‹œí”¼ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ë‹µë³€)")
        else:
            print("ğŸ¤– LLM ì „ìš© ëª¨ë“œ (ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€)")
        print("="*60)
        print("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥).\n")
        
        while True:
            try:
                question = input("\nì§ˆë¬¸: ")
                if question is None:
                    continue
                question = question.strip()
            except EOFError:
                print("\nì…ë ¥ ìŠ¤íŠ¸ë¦¼ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except KeyboardInterrupt:
                print("\n\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if question.lower() == 'exit':
                print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ë¹ˆ ì§ˆë¬¸ ì²˜ë¦¬
            if not question:
                print("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ì§ˆë¬¸ì—ì„œ ì•Œë ˆë¥´ê¸° ì •ë³´ ì¶”ì¶œ
            user_allergens = AllergenExtractor.extract_from_query(question)
            search_query = question
            
            if user_allergens:
                self.retriever.set_allergens(user_allergens)
                # ì•Œë ˆë¥´ê¸° í‚¤ì›Œë“œë¥¼ ì œê±°í•œ ê¹¨ë—í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                search_query = AllergenExtractor.remove_allergen_keywords(question, user_allergens)
                print(f"\nğŸ” ì•Œë ˆë¥´ê¸° ì •ë³´ ê°ì§€: {', '.join(user_allergens)}")
                print(f"â†’ í•´ë‹¹ ì¬ë£Œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ë ˆì‹œí”¼ë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
                print(f"â†’ ê²€ìƒ‰ ì¿¼ë¦¬: \"{search_query}\"\n")
            else:
                self.retriever.set_allergens([])
            
            if mode == 'compare':
                # RAG ë‹µë³€
                print("\n" + "="*60)
                print("ğŸ” RAG ëª¨ë“œ ë‹µë³€ (ë ˆì‹œí”¼ DB ê¸°ë°˜):")
                print("="*60)
                start_time = time.time()
                rag_answer = self.rag_chain.invoke({"question": question, "search_query": search_query})
                rag_time = time.time() - start_time
                print(f"\n{rag_answer}")
                print(f"\nâ±ï¸ ì‘ë‹µ ì‹œê°„: {rag_time:.2f}ì´ˆ")
                
                # LLM ì „ìš© ë‹µë³€
                print("\n" + "="*60)
                print("ğŸ¤– LLM ì „ìš© ëª¨ë“œ ë‹µë³€ (ì¼ë°˜ ì§€ì‹ ê¸°ë°˜):")
                print("="*60)
                start_time = time.time()
                llm_answer = self.llm_only_chain.invoke(question)
                llm_time = time.time() - start_time
                print(f"\n{llm_answer}")
                print(f"\nâ±ï¸ ì‘ë‹µ ì‹œê°„: {llm_time:.2f}ì´ˆ")
                
                # ë¹„êµ ìš”ì•½
                print("\n" + "="*60)
                print("ğŸ“Š ë¹„êµ ìš”ì•½:")
                print("="*60)
                print(f"RAG ëª¨ë“œ: ë ˆì‹œí”¼ DBì—ì„œ ê²€ìƒ‰ëœ ì‹¤ì œ ë ˆì‹œí”¼ ì •ë³´ ì œê³µ")
                print(f"LLM ëª¨ë“œ: ì¼ë°˜ì ì¸ ìš”ë¦¬ ì§€ì‹ ê¸°ë°˜ ë‹µë³€")
                print(f"ì‘ë‹µ ì‹œê°„ ì°¨ì´: {abs(rag_time - llm_time):.2f}ì´ˆ")
                
            elif mode == 'rag':
                # RAGë§Œ ì‹¤í–‰
                start_time = time.time()
                answer = self.rag_chain.invoke({"question": question, "search_query": search_query})
                elapsed_time = time.time() - start_time
                print(f"\në‹µë³€: {answer}")
                print(f"\nâ±ï¸ ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                
            else:  # llm
                # LLMë§Œ ì‹¤í–‰
                start_time = time.time()
                answer = self.llm_only_chain.invoke(question)
                elapsed_time = time.time() - start_time
                print(f"\në‹µë³€: {answer}")
                print(f"\nâ±ï¸ ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            
            # í”¼ë“œë°± ìˆ˜ì§‘ (RAG ëª¨ë“œ ë˜ëŠ” ë¹„êµ ëª¨ë“œì¼ ë•Œë§Œ)
            if mode in ['rag', 'compare']:
                print("\n" + "="*60)
                print("ì´ ë‹µë³€ì´ ë„ì›€ì´ ë˜ì—ˆë‚˜ìš”?")
                print("ğŸ‘ ì¢‹ì•„ìš” (1) | ğŸ‘ ë³„ë¡œì—ìš” (2) | â­ï¸  ê±´ë„ˆë›°ê¸° (Enter)")
                feedback_input = input("ì„ íƒ: ").strip()
                
                if feedback_input in ['1', '2']:
                    docs = self.retriever.invoke(search_query)
                    if docs:
                        doc = docs[0]
                        recipe_title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
                        recipe_url = doc.metadata.get('source', '')
                        is_positive = (feedback_input == '1')
                        self.feedback_store.add_feedback(question, recipe_title, recipe_url, is_positive)
                else:
                    print("í”¼ë“œë°±ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ³ ë ˆì‹œí”¼ ì±—ë´‡")
    print("="*60)
    print("\nëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ë¹„êµ ëª¨ë“œ (RAG vs LLM ì „ìš©) - ê¸°ë³¸ê°’")
    print("2. RAG ëª¨ë“œ (ë ˆì‹œí”¼ DB ê¸°ë°˜)")
    print("3. LLM ì „ìš© ëª¨ë“œ (ì¼ë°˜ ì§€ì‹ ê¸°ë°˜)")
    
    mode_input = input("\nì„ íƒ (1-3, Enter=ë¹„êµëª¨ë“œ): ").strip()
    
    if mode_input == '2':
        mode = 'rag'
    elif mode_input == '3':
        mode = 'llm'
    else:
        mode = 'compare'
    
    # 100% LLM ê¸°ë°˜ ì•Œë ˆë¥´ê¸° íƒì§€
    print(f"\nâš™ï¸ ì•Œë ˆë¥´ê¸° íƒì§€ ëª¨ë“œ: 100% LLM ê¸°ë°˜ ë¶„ì„ (ìµœê³  ì •í™•ë„)")

    
    chatbot = RagChatbot(
        csv_path="data/recipe_main_5.csv",
        faiss_index_path="faiss_recipe_index"
    )
    chatbot.run(mode=mode)


if __name__ == "__main__":
    main()
