import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("=" * 80)
print("ğŸ“Š ë ˆì‹œí”¼ ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸")
print("=" * 80)

# 1. ê¸°ë³¸ ë°ì´í„° ì •ë³´
print("\n[1] ê¸°ë³¸ ë°ì´í„° ì •ë³´")
print("-" * 80)

df = pd.read_csv('data/recipe_main_5.csv', encoding='utf-8', low_memory=False)
print(f"âœ… ì´ ë ˆì‹œí”¼ ìˆ˜: {len(df):,}ê°œ")
print(f"âœ… ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
print(f"\nğŸ“‹ ì»¬ëŸ¼ ëª©ë¡:")
for i, col in enumerate(df.columns, 1):
    print(f"  {i:2d}. {col}")

# 2. ê²°ì¸¡ì¹˜ ë¶„ì„
print("\n\n[2] ê²°ì¸¡ì¹˜ ë¶„ì„")
print("-" * 80)
missing = df.isnull().sum()
missing_pct = (missing / len(df) * 100).round(2)
missing_df = pd.DataFrame({
    'ê²°ì¸¡ì¹˜ ìˆ˜': missing,
    'ê²°ì¸¡ ë¹„ìœ¨(%)': missing_pct
})
missing_df = missing_df[missing_df['ê²°ì¸¡ì¹˜ ìˆ˜'] > 0].sort_values('ê²°ì¸¡ì¹˜ ìˆ˜', ascending=False)
if len(missing_df) > 0:
    print(missing_df)
else:
    print("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")

# 3. ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
print("\n\n[3] ì¹´í…Œê³ ë¦¬ë³„ ë ˆì‹œí”¼ ë¶„í¬")
print("-" * 80)

categories = ['ì¢…ë¥˜ë³„', 'ìƒí™©ë³„', 'ì¬ë£Œë³„', 'ë°©ë²•ë³„']
for cat in categories:
    if cat in df.columns:
        print(f"\nğŸ“Œ {cat} ë¶„í¬:")
        value_counts = df[cat].value_counts().head(10)
        for idx, (value, count) in enumerate(value_counts.items(), 1):
            pct = (count / len(df) * 100)
            bar = 'â–ˆ' * int(pct / 2)
            print(f"  {idx:2d}. {str(value):20s} â”‚ {count:6,}ê°œ ({pct:5.2f}%) {bar}")

# 4. ë‚œì´ë„ ë¶„í¬
print("\n\n[4] ë‚œì´ë„ ë¶„í¬")
print("-" * 80)
if 'ë‚œì´ë„' in df.columns:
    difficulty = df['ë‚œì´ë„'].value_counts()
    for level, count in difficulty.items():
        pct = (count / len(df) * 100)
        bar = 'â–ˆ' * int(pct / 5)
        print(f"  {str(level):10s} â”‚ {count:6,}ê°œ ({pct:5.2f}%) {bar}")

# 5. ì¡°ë¦¬ì‹œê°„ ë¶„ì„
print("\n\n[5] ì¡°ë¦¬ì‹œê°„ ë¶„ì„")
print("-" * 80)
if 'ì¡°ë¦¬ì‹œê°„' in df.columns:
    time_counts = df['ì¡°ë¦¬ì‹œê°„'].value_counts().head(15)
    for time, count in time_counts.items():
        pct = (count / len(df) * 100)
        bar = 'â–ˆ' * int(pct / 2)
        print(f"  {str(time):20s} â”‚ {count:6,}ê°œ ({pct:5.2f}%) {bar}")

# 6. ì¸ë¶„ ë¶„ì„
print("\n\n[6] ì¸ë¶„ ë¶„ì„")
print("-" * 80)
if 'ì¸ë¶„' in df.columns:
    servings = df['ì¸ë¶„'].value_counts().head(10)
    for serving, count in servings.items():
        pct = (count / len(df) * 100)
        bar = 'â–ˆ' * int(pct / 2)
        print(f"  {str(serving):10s} â”‚ {count:6,}ê°œ ({pct:5.2f}%) {bar}")

# 7. ì¡°íšŒìˆ˜ ë¶„ì„
print("\n\n[7] ì¡°íšŒìˆ˜ í†µê³„")
print("-" * 80)
if 'ì¡°íšŒìˆ˜' in df.columns:
    views = pd.to_numeric(df['ì¡°íšŒìˆ˜'], errors='coerce')
    print(f"  í‰ê·  ì¡°íšŒìˆ˜:   {views.mean():,.0f}íšŒ")
    print(f"  ì¤‘ì•™ê°’:        {views.median():,.0f}íšŒ")
    print(f"  ìµœì†Œ ì¡°íšŒìˆ˜:   {views.min():,.0f}íšŒ")
    print(f"  ìµœëŒ€ ì¡°íšŒìˆ˜:   {views.max():,.0f}íšŒ")
    print(f"  í‘œì¤€í¸ì°¨:      {views.std():,.0f}íšŒ")
    
    # ì¡°íšŒìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
    print(f"\n  ğŸ“Š ì¡°íšŒìˆ˜ êµ¬ê°„ë³„ ë¶„í¬:")
    max_view = views.max()
    if max_view <= 1000:
        bins = [0, 200, 400, 600, 800, max_view + 1]
        labels = ['~200', '200~400', '400~600', '600~800', '800~']
    else:
        bins = [0, 1000, 5000, 10000, 50000, 100000, max_view + 1]
        labels = ['~1ì²œ', '1ì²œ~5ì²œ', '5ì²œ~1ë§Œ', '1ë§Œ~5ë§Œ', '5ë§Œ~10ë§Œ', '10ë§Œ~']
    
    views_binned = pd.cut(views, bins=bins, labels=labels)
    for label, count in views_binned.value_counts().sort_index().items():
        pct = (count / len(views) * 100)
        bar = 'â–ˆ' * int(pct / 2)
        print(f"    {str(label):10s} â”‚ {count:6,}ê°œ ({pct:5.2f}%) {bar}")

# 8. ì…°í”„ë³„ ë ˆì‹œí”¼ ìˆ˜
print("\n\n[8] ì…°í”„ë³„ ë ˆì‹œí”¼ ìˆ˜ (ìƒìœ„ 15ëª…)")
print("-" * 80)
if 'ì…°í”„' in df.columns:
    chefs = df['ì…°í”„'].value_counts().head(15)
    for idx, (chef, count) in enumerate(chefs.items(), 1):
        pct = (count / len(df) * 100)
        bar = 'â–ˆ' * int(pct / 2)
        print(f"  {idx:2d}. {str(chef):30s} â”‚ {count:5,}ê°œ ({pct:5.2f}%) {bar}")

# 9. ì¬ë£Œ ë¶„ì„
print("\n\n[9] ì¬ë£Œ ë¶„ì„")
print("-" * 80)
if 'ì¬ë£Œ' in df.columns:
    # ì¬ë£Œ ê¸¸ì´ í†µê³„
    ingredient_lengths = df['ì¬ë£Œ'].str.len()
    print(f"  í‰ê·  ì¬ë£Œ í…ìŠ¤íŠ¸ ê¸¸ì´: {ingredient_lengths.mean():.0f}ì")
    print(f"  ì¤‘ì•™ê°’:               {ingredient_lengths.median():.0f}ì")
    print(f"  ìµœì†Œ ê¸¸ì´:            {ingredient_lengths.min():.0f}ì")
    print(f"  ìµœëŒ€ ê¸¸ì´:            {ingredient_lengths.max():.0f}ì")
    
    # ì¬ë£Œ ê°œìˆ˜ ì¶”ì • (ì‰¼í‘œ ê¸°ì¤€)
    ingredient_counts = df['ì¬ë£Œ'].str.count(',') + 1
    print(f"\n  í‰ê·  ì¬ë£Œ ê°œìˆ˜ (ì¶”ì •): {ingredient_counts.mean():.1f}ê°œ")
    print(f"  ì¤‘ì•™ê°’:               {ingredient_counts.median():.0f}ê°œ")
    print(f"  ìµœì†Œ ê°œìˆ˜:            {ingredient_counts.min():.0f}ê°œ")
    print(f"  ìµœëŒ€ ê°œìˆ˜:            {ingredient_counts.max():.0f}ê°œ")

# 10. ì•Œë ˆë¥´ê¸° ë¶„ì„ (FAISS ì¸ë±ìŠ¤ì—ì„œ)
print("\n\n[10] ì•Œë ˆë¥´ê¸° ì •ë³´ ë¶„ì„")
print("-" * 80)

try:
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    
    print("  FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embedments': True}
    )
    
    vectorstore = FAISS.load_local(
        "faiss_recipe_index",
        embeddings,
        allow_dangerous_deserialization=True
    )
    
    # ìƒ˜í”Œ ë¬¸ì„œì—ì„œ ì•Œë ˆë¥´ê¸° ì •ë³´ ìˆ˜ì§‘
    print("  ì•Œë ˆë¥´ê¸° ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    sample_docs = vectorstore.similarity_search("", k=1000)
    
    allergen_counter = Counter()
    docs_with_allergens = 0
    docs_without_allergens = 0
    
    for doc in sample_docs:
        allergens = doc.metadata.get('allergens', [])
        if allergens:
            docs_with_allergens += 1
            for allergen in allergens:
                allergen_counter[allergen] += 1
        else:
            docs_without_allergens += 1
    
    print(f"\n  âœ… ì•Œë ˆë¥´ê¸° ì •ë³´ ìˆëŠ” ë¬¸ì„œ: {docs_with_allergens}ê°œ")
    print(f"  âš ï¸  ì•Œë ˆë¥´ê¸° ì •ë³´ ì—†ëŠ” ë¬¸ì„œ: {docs_without_allergens}ê°œ")
    
    if allergen_counter:
        print(f"\n  ğŸ“Š ì•Œë ˆë¥´ê¸° ì„±ë¶„ë³„ ë¹ˆë„ (ìƒìœ„ 19ê°œ):")
        for idx, (allergen, count) in enumerate(allergen_counter.most_common(19), 1):
            pct = (count / len(sample_docs) * 100)
            bar = 'â–ˆ' * int(pct / 2)
            print(f"    {idx:2d}. {allergen:10s} â”‚ {count:5,}íšŒ ({pct:5.2f}%) {bar}")

except Exception as e:
    print(f"  âš ï¸  FAISS ì¸ë±ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
    print("  (ë²¡í„° ì €ì¥ì†Œê°€ ì•„ì§ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

# 11. í•´ì‹œíƒœê·¸ ë¶„ì„
print("\n\n[11] í•´ì‹œíƒœê·¸ ë¶„ì„ (ìƒìœ„ 20ê°œ)")
print("-" * 80)
if 'í•´ì‹œíƒœê·¸' in df.columns:
    all_hashtags = []
    for tags in df['í•´ì‹œíƒœê·¸'].dropna():
        if isinstance(tags, str):
            # #ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤ ì¶”ì¶œ
            hashtags = [tag.strip() for tag in str(tags).split() if tag.startswith('#')]
            all_hashtags.extend(hashtags)
    
    if all_hashtags:
        hashtag_counter = Counter(all_hashtags)
        for idx, (tag, count) in enumerate(hashtag_counter.most_common(20), 1):
            pct = (count / len(all_hashtags) * 100)
            bar = 'â–ˆ' * int(pct)
            print(f"  {idx:2d}. {tag:20s} â”‚ {count:5,}íšŒ ({pct:5.2f}%) {bar}")
    else:
        print("  í•´ì‹œíƒœê·¸ ì •ë³´ ì—†ìŒ")

# 12. AI ë¦¬ë·° ìš”ì•½ ë¶„ì„
print("\n\n[12] AI ë¦¬ë·° ìš”ì•½ ì •ë³´")
print("-" * 80)
if 'AIë¦¬ë·°ìš”ì•½' in df.columns:
    has_review = df['AIë¦¬ë·°ìš”ì•½'].notna().sum()
    no_review = df['AIë¦¬ë·°ìš”ì•½'].isna().sum()
    pct_has = (has_review / len(df) * 100)
    pct_no = (no_review / len(df) * 100)
    
    print(f"  âœ… AI ë¦¬ë·° ìˆìŒ: {has_review:,}ê°œ ({pct_has:.2f}%)")
    print(f"  âŒ AI ë¦¬ë·° ì—†ìŒ: {no_review:,}ê°œ ({pct_no:.2f}%)")

# 13. í”¼ë“œë°± ë°ì´í„° ë¶„ì„
print("\n\n[13] ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„")
print("-" * 80)

try:
    with open('feedback_data.json', 'r', encoding='utf-8') as f:
        feedback_data = json.load(f)
    
    positive_count = len(feedback_data.get('positive', []))
    negative_count = len(feedback_data.get('negative', []))
    total_feedback = positive_count + negative_count
    
    if total_feedback > 0:
        pos_pct = (positive_count / total_feedback * 100)
        neg_pct = (negative_count / total_feedback * 100)
        
        print(f"  ì´ í”¼ë“œë°± ìˆ˜: {total_feedback}ê°œ")
        print(f"  ğŸ‘ ê¸ì •: {positive_count}ê°œ ({pos_pct:.1f}%)")
        print(f"  ğŸ‘ ë¶€ì •: {negative_count}ê°œ ({neg_pct:.1f}%)")
        
        # í”¼ë“œë°± ë§ì€ ë ˆì‹œí”¼
        if positive_count > 0:
            print(f"\n  ğŸ“Œ ê¸ì • í”¼ë“œë°± ë°›ì€ ë ˆì‹œí”¼:")
            positive_recipes = Counter([fb['recipe_title'] for fb in feedback_data['positive']])
            for idx, (recipe, count) in enumerate(positive_recipes.most_common(5), 1):
                print(f"    {idx}. {recipe[:50]} ({count}íšŒ)")
    else:
        print("  âš ï¸  í”¼ë“œë°± ë°ì´í„° ì—†ìŒ")
        
except Exception as e:
    print(f"  âš ï¸  í”¼ë“œë°± íŒŒì¼ ì—†ìŒ ë˜ëŠ” ì½ê¸° ì‹¤íŒ¨: {e}")

# 14. ë°ì´í„° í’ˆì§ˆ í‰ê°€
print("\n\n[14] ë°ì´í„° í’ˆì§ˆ í‰ê°€")
print("-" * 80)

quality_score = 0
max_score = 0

# í•„ìˆ˜ ì»¬ëŸ¼ ì™„ì„±ë„
required_cols = ['ì œëª©', 'ì¬ë£Œ', 'ì¡°ë¦¬ìˆœì„œ', 'url']
for col in required_cols:
    max_score += 1
    if col in df.columns:
        completeness = (df[col].notna().sum() / len(df) * 100)
        print(f"  {col:15s} ì™„ì„±ë„: {completeness:6.2f}%")
        if completeness > 95:
            quality_score += 1
        elif completeness > 80:
            quality_score += 0.7
        elif completeness > 50:
            quality_score += 0.5

# ë©”íƒ€ë°ì´í„° í’ë¶€ë„
metadata_cols = ['ë‚œì´ë„', 'ì¡°ë¦¬ì‹œê°„', 'ì¸ë¶„', 'ì…°í”„', 'ì¡°íšŒìˆ˜']
for col in metadata_cols:
    max_score += 1
    if col in df.columns:
        completeness = (df[col].notna().sum() / len(df) * 100)
        if completeness > 80:
            quality_score += 1
        elif completeness > 50:
            quality_score += 0.7

final_quality = (quality_score / max_score * 100)
print(f"\n  ğŸ“Š ì „ì²´ ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {final_quality:.1f}/100ì ")

if final_quality >= 90:
    grade = "A+ (ìš°ìˆ˜)"
elif final_quality >= 80:
    grade = "A (ì–‘í˜¸)"
elif final_quality >= 70:
    grade = "B+ (ë³´í†µ)"
else:
    grade = "B (ê°œì„  í•„ìš”)"

print(f"  ğŸ† ë“±ê¸‰: {grade}")

# ìš”ì•½
print("\n\n" + "=" * 80)
print("ğŸ“Š ë¶„ì„ ìš”ì•½")
print("=" * 80)
print(f"âœ… ì´ {len(df):,}ê°œì˜ ë ˆì‹œí”¼ ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
print(f"âœ… {len(df.columns)}ê°œì˜ ì»¬ëŸ¼ í™•ì¸")
print(f"âœ… ë°ì´í„° í’ˆì§ˆ: {final_quality:.1f}ì  ({grade})")
print("=" * 80)
